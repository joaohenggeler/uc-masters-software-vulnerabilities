#!/usr/bin/env python3

"""
	This script defines a class that specifies how Propheticus should batch process the datasets generated by "build_dataset_from_database.py".
"""

from copy import deepcopy

import propheticus.core.BatchExecution

from InstanceConfig import InstanceConfig
from modules.common import log

class InstanceBatchExecution(propheticus.core.BatchExecution):

	def __init__(self, context):
		super(InstanceBatchExecution, self).__init__(context)

		# self.display_visuals = False

		self.Processes = [
			{'method': 'DataAnalysis', 'arguments': ['lineGraphs']},
			{'method': 'DataAnalysis', 'arguments': ['boxPlots']},
			{'method': 'DataAnalysis', 'arguments': ['correlationMatrixPlot']},
			{'method': 'parseCurrentConfigurationAlgorithms', 'arguments': [None, True]},
		]

		"""
		The dimensionality reduction techniques are specified in 'propheticus/configs/DimensionalityReduction.py'.
		The sampling techniques are specified in 'propheticus/configs/Sampling.py'.
		The algorithms and their parameters are specified in 'propheticus/configs/Classification.py'.

		Algorithm Parameters:

		# Random Forests:

		'n_estimators': {'type': 'int', 'default': 100},
		'criterion': {'type': 'str', 'values': ['gini', 'entropy']},
		'max_depth': {'type': ['int', 'None']},
		'min_samples_split': {'type': ['int', 'float']},
		'min_samples_leaf': {'type': ['int', 'float']},
		'min_weight_fraction_leaf': {'type': 'float'},
		'max_features': {'type': ['int', 'float', 'str', 'None']},
		'max_leaf_nodes': {'type': ['int', 'None']},
		'min_impurity_decrease': {'type': 'float'},
		'min_impurity_split': {'type': 'float'},
		'bootstrap': {'type': 'bool', 'values': [True, False]},
		'oob_score': {'type': 'bool', 'values': [True, False]},
		'n_jobs': {'type': ['int', 'None'], 'default': -1},
		'random_state': {'hide': True, 'type': ['int', 'None']},
		'verbose': {'type': 'int'},
		'warm_start': {'type': 'bool', 'values': [True, False]},
		'class_weight': {'type': ['dict', 'list-dicts', 'balanced', 'None']}

		# Bagging:

		'base_estimator': {'type': ''},
		'n_estimators': {'type': '', 'default': 100},
		'max_samples': {'type': ''},
		'max_features': {'type': ''},
		'bootstrap': {'type': ''},
		'bootstrap_features': {'type': ''},
		'oob_score': {'type': ''},
		'warm_start': {'type': ''},
		'n_jobs': {'type': ''},
		'random_state': {'hide': True, 'type': ''},
		'verbose': {'type': ''}

		# XGBoost:

		'n_estimators': {'type': 'int'},
		'learning_rate': {'type': 'float'},
		'max_depth': {'type': 'int'},
		'subsample': {'type': 'float'},
		'objective': {'type': 'str'},
		'gamma': {'type': 'float'},
		'alpha': {'type': 'float'},
		'lambda': {'type': 'float'},
		'random_state': {'hide': True, 'type': ''},
		"""

		base_configuration = {}
		base_configuration['config_seed_count'] = InstanceConfig.PROPHETICUS['seed_count']
		# Deprecated (see the config): base_configuration['config_cv_fold'] = 5
		base_configuration['config_data_split_parameters'] = InstanceConfig.PROPHETICUS['data_split']
		base_configuration['config_grid_search'] = False
		base_configuration['config_binary_classification'] = False
		
		# This is only used when 'config_binary_classification' is false.
		prediction_classes = list(InstanceConfig.ClassesDescription.values())
		base_configuration['datasets_positive_classes'] = prediction_classes[1:]

		for code_unit in InstanceConfig.CODE_UNIT_LIST:

			dataset_list = [f'{project.short_name}.{code_unit}' for project in InstanceConfig.PROJECT_LIST]
	
			for target_label in InstanceConfig.PROPHETICUS['labels']:
				
				excluded_label_list = [label for label in InstanceConfig.PROPHETICUS['labels'] if label != target_label]

				for dimensionality_reduction in InstanceConfig.PROPHETICUS['dimensionality_reduction']:
					
					for data_balancing in InstanceConfig.PROPHETICUS['data_balancing']:
							
						# From the original demo:
						# TODO: improve the following logic to allow passing more than one algorithm to improve performance
						# TODO: use something similar to itertools but be aware of different lenghts in configs; itertools.product stops at the minimum length
						for classification_algorithm, algorithm_parameter_list in InstanceConfig.PROPHETICUS['classification_algorithms'].items():
							
							for algorithm_parameter in algorithm_parameter_list:
								
								configuration = deepcopy(base_configuration)

								configuration['datasets'] = dataset_list

								configuration['pre_target'] = target_label
								configuration['pre_excluded_features'] = excluded_label_list

								configuration['proc_reduce_dimensionality'] = dimensionality_reduction
								configuration['proc_balance_data'] = data_balancing

								configuration['proc_classification'] = [classification_algorithm]

								configuration['proc_classification_algorithms_parameters'] = {}
								if algorithm_parameter is not None:
									configuration['proc_classification_algorithms_parameters'] = {classification_algorithm: algorithm_parameter}
								
								log.info(f'Adding the batch configuration: {dataset_list}, {target_label}, {dimensionality_reduction}, {data_balancing}, {classification_algorithm}, {algorithm_parameter_list}')
								self.Configurations.append(configuration)

		log.info(f'Added a total of {len(self.Configurations)} batch configurations.')