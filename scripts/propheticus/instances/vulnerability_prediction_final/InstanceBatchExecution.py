"""
Contains the code required to perform batch execution of the framework
"""

from copy import deepcopy

import propheticus.core.BatchExecution

from InstanceConfig import InstanceConfig
from modules.common import log

class InstanceBatchExecution(propheticus.core.BatchExecution):
	"""
	Contains the code required to perform batch execution of the framework

	...

	Attributes
	----------
	"""
	def __init__(self, context):
		super(InstanceBatchExecution, self).__init__(context)

		# self.display_visuals = False

		self.Processes = [
			{'method': 'DataAnalysis', 'arguments': ['lineGraphs']},
			{'method': 'DataAnalysis', 'arguments': ['boxPlots']},
			{'method': 'DataAnalysis', 'arguments': ['correlationMatrixPlot']},
			{'method': 'parseCurrentConfigurationAlgorithms', 'arguments': [None, True]},
		]

		# The dimensionality reduction techniques are specified in 'propheticus/configs/DimensionalityReduction.py'.
		dimensionality_reduction_list = [
			['variance', 'correlation'],
			['variance', 'pca'],
			['correlation', 'pca'],
		]

		# The sampling techniques are specified in 'propheticus/configs/Sampling.py'.
		data_balancing_list = [
			['RandomUnderSampler'],
			['RandomUnderSampler', 'RandomOverSampler'],
			['RandomUnderSampler', 'SMOTE'],
		]

		filter_features = [
			None
		]

		"""
			'random_forests': propheticus.shared.Utils.cartesianProductDictionaryLists(
				#n_estimators=[10, 50, 100, 200],
				n_estimators=[100],
				criterion=['gini'],
				#min_samples_split=[0.001, 2],
				min_samples_split=[2],
				#min_samples_leaf=[0.001, 1],
				min_samples_leaf=[1],
				#max_features=list(numpy.linspace(0.1, 1.0, 3)) + [None],
				max_features=[None],
				#bootstrap=[True, False]
				bootstrap=[True]
			),

			'svm': 	propheticus.shared.Utils.cartesianProductDictionaryLists(kernel=['linear'], C=[1, 0.1, 0.01]) +
				propheticus.shared.Utils.cartesianProductDictionaryLists(kernel=['rbf'], C=[1, 0.1, 0.01], gamma=[0.1, 1, 10]) +
				propheticus.shared.Utils.cartesianProductDictionaryLists(kernel=['poly'], C=[1, 0.1, 0.01], gamma=[0.1, 1], degree=range(2, 4, 2)),

			'xgboost': propheticus.shared.Utils.cartesianProductDictionaryLists(
				n_estimators=[100],
				learning_rate=[0.5, 0.3, 0.1],
				gamma=[0, 0.1, 0.3],
				subsample=[0.5, 0.7, 1]
			),
		"""

		"""
		Algorithm Parameters:

		# Random Forests:

		'n_estimators': {'type': 'int', 'default': 100},
		'criterion': {'type': 'str', 'values': ['gini', 'entropy']},
		'max_depth': {'type': ['int', 'None']},
		'min_samples_split': {'type': ['int', 'float']},
		'min_samples_leaf': {'type': ['int', 'float']},
		'min_weight_fraction_leaf': {'type': 'float'},
		'max_features': {'type': ['int', 'float', 'str', 'None']},
		'max_leaf_nodes': {'type': ['int', 'None']},
		'min_impurity_decrease': {'type': 'float'},
		'min_impurity_split': {'type': 'float'},
		'bootstrap': {'type': 'bool', 'values': [True, False]},
		'oob_score': {'type': 'bool', 'values': [True, False]},
		'n_jobs': {'type': ['int', 'None'], 'default': -1},
		'random_state': {'hide': True, 'type': ['int', 'None']},
		'verbose': {'type': 'int'},
		'warm_start': {'type': 'bool', 'values': [True, False]},
		'class_weight': {'type': ['dict', 'list-dicts', 'balanced', 'None']}

		# Bagging:

		'base_estimator': {'type': ''},
		'n_estimators': {'type': '', 'default': 100},
		'max_samples': {'type': ''},
		'max_features': {'type': ''},
		'bootstrap': {'type': ''},
		'bootstrap_features': {'type': ''},
		'oob_score': {'type': ''},
		'warm_start': {'type': ''},
		'n_jobs': {'type': ''},
		'random_state': {'hide': True, 'type': ''},
		'verbose': {'type': ''}

		# XGBoost:

		'n_estimators': {'type': 'int'},
		'learning_rate': {'type': 'float'},
		'max_depth': {'type': 'int'},
		'subsample': {'type': 'float'},
		'objective': {'type': 'str'},
		'gamma': {'type': 'float'},
		'alpha': {'type': 'float'},
		'lambda': {'type': 'float'},
		'random_state': {'hide': True, 'type': ''},
		"""

		# The algorithms and their parameters are specified in 'propheticus/configs/Classification.py'.
		classification_algorithm_info = {
			'random_forests': [None],
			'bagging': [None],
			'xgboost': [None],
		}

		undersampling_threshold_by_algorithm = {
			'default': 10000
		}

		base_configuration = {}
		base_configuration['config_seed_count'] = 30
		# Deprecated (see below): base_configuration['config_cv_fold'] = 5
		base_configuration['config_data_split_parameters'] = {'n_splits': 10}
		base_configuration['config_grid_search'] = False
		base_configuration['config_binary_classification'] = False
		
		# This is only used when 'config_binary_classification' is false.
		prediction_classes = list(InstanceConfig.ClassesDescription.values())
		base_configuration['datasets_positive_classes'] = prediction_classes[1:]

		label_list = ['binary_label', 'grouped_multiclass_label']

		for code_unit in ['file', 'function', 'class']:

			dataset_list = [f'{project.short_name}.{code_unit}' for project in InstanceConfig.PROJECT_LIST]
			log.info(f'Adding the batch configuration using the datasets: {dataset_list}')

			for target_label in label_list:
				
				excluded_label_list = [label for label in label_list if label != target_label]

				for dimensionality_reduction in dimensionality_reduction_list:
					
					for data_balancing in data_balancing_list:
						
						for filter_list in filter_features:
							
							# From the original demo:
							# TODO: improve the following logic to allow passing more than one algorithm to improve performance
							# TODO: use something similar to itertools but be aware of different lenghts in configs; itertools.product stops at the minimum length
							for classification_algorithm, algorithm_parameter_list in classification_algorithm_info.items():
								
								for algorithm_parameter in algorithm_parameter_list:
									
									configuration = deepcopy(base_configuration)

									configuration['datasets'] = dataset_list

									configuration['pre_target'] = target_label
									configuration['pre_excluded_features'] = excluded_label_list

									configuration['proc_reduce_dimensionality'] = dimensionality_reduction
									configuration['proc_balance_data'] = data_balancing

									configuration['pre_filter_feature_values'] = []
									if filter_list is not None:
										for filter in filter_list:
											filter['label'] += ': ' + filter['values']
										configuration['pre_filter_feature_values'] = filter_list

									configuration['proc_classification'] = [classification_algorithm]
									configuration['config_undersampling_threshold'] = undersampling_threshold_by_algorithm[classification_algorithm] if classification_algorithm in undersampling_threshold_by_algorithm else undersampling_threshold_by_algorithm['default']

									configuration['proc_classification_algorithms_parameters'] = {}
									if algorithm_parameter is not None:
										configuration['proc_classification_algorithms_parameters'] = {classification_algorithm: algorithm_parameter}
									
									self.Configurations.append(configuration)

		log.info(f'Added a total of {len(self.Configurations)} batch configurations.')