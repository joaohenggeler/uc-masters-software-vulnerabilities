"""
Contains the code required to perform batch execution of the framework
"""

from copy import deepcopy

import propheticus.core.BatchExecution

from InstanceConfig import InstanceConfig
from modules.common import log

class InstanceBatchExecution(propheticus.core.BatchExecution):
	"""
	Contains the code required to perform batch execution of the framework

	...

	Attributes
	----------
	"""
	def __init__(self, context):
		super(InstanceBatchExecution, self).__init__(context)

		# self.display_visuals = False

		self.Processes = [
			{'method': 'parseCurrentConfigurationAlgorithms', 'arguments': [None, True]},
			# oUI.dataAnalysis('boxPlots'),
			# oUI.dataAnalysis('lineGraphs'),
			# oUI.dataAnalysis('correlationMatrixPlot'),
			# oUI.dataAnalysis('descriptiveAnalysis'),
		]

		binary_classification_list = [
			# True,
			False
		]

		# The dimensionality reduction techniques are specified in 'propheticus/configs/DimensionalityReduction.py'.
		dimensionality_reduction_list = [
			['variance'],
			# ['variance', 'correlation']
		]

		# The sampling techniques are specified in 'propheticus/configs/Sampling.py'.
		data_balancing_list = [
			[],
			['RandomUnderSampler'],
			['RandomUnderSampler', 'SMOTE'],
		]

		filter_features = [
			None
		]

		# The algorithms and their parameters are specified in 'propheticus/configs/Classification.py'.
		classification_algorithm_info = {
			'decision_tree': [None]
			# 'random_forests': propheticus.shared.Utils.cartesianProductDictionaryLists(
			#	 n_estimators=[20, 100],
			#	 criterion=['entropy', 'gini']
			# )
		}

		undersampling_threshold_by_algorithm = {
			'default': 10000
		}

		base_configuration = {}
		base_configuration['config_seed_count'] = 30
		base_configuration['config_cv_fold'] = 5
		base_configuration['config_grid_search'] = False
		
		# This is only used when 'config_binary_classification' is false.
		prediction_classes = list(InstanceConfig.ClassesDescription.values())
		base_configuration['datasets_positive_classes'] = prediction_classes[1:]

		for code_unit in ['files', 'functions', 'classes']:

			dataset_list = [f'{project.short_name}.{code_unit}' for project in InstanceConfig.PROJECT_LIST]
			log.info(f'Adding the batch configuration using the datasets: {dataset_list}')

			for binary_classification in binary_classification_list:
				
				for dimensionality_reduction in dimensionality_reduction_list:
					
					for data_balancing in data_balancing_list:
						
						for filter_list in filter_features:
							
							# From the original demo:
							# TODO: improve the following logic to allow passing more than one algorithm to improve performance
							# TODO: use something similar to itertools but be aware of different lenghts in configs; itertools.product stops at the minimum length
							for classification_algorithm, algorithm_parameter_list in classification_algorithm_info.items():
								
								for algorithm_parameter in algorithm_parameter_list:
									
									configuration = deepcopy(base_configuration)

									configuration['datasets'] = dataset_list
									configuration['config_binary_classification'] = binary_classification
									configuration['proc_reduce_dimensionality'] = dimensionality_reduction
									configuration['proc_balance_data'] = data_balancing

									configuration['pre_filter_feature_values'] = []
									if filter_list is not None:
										for filter in filter_list:
											filter['label'] += ': ' + filter['values']
										configuration['pre_filter_feature_values'] = filter_list

									configuration['proc_classification'] = [classification_algorithm]
									configuration['config_undersampling_threshold'] = undersampling_threshold_by_algorithm[classification_algorithm] if classification_algorithm in undersampling_threshold_by_algorithm else undersampling_threshold_by_algorithm['default']

									configuration['proc_classification_algorithms_parameters'] = {}
									if algorithm_parameter is not None:
										configuration['proc_classification_algorithms_parameters'] = {classification_algorithm: algorithm_parameter}
									
									self.Configurations.append(configuration)