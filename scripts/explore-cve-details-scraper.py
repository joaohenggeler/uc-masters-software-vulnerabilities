#!/usr/bin/env python
import sys
import requests
import bs4
import re
import csv

"""
	This script explores the information available in the CVE Details website by scraping every page related to the Mozilla vendor,
	and by storing the information about each CVE in a CSV file. No connections to the software vulnerabilities database are made.

	Requirements:

	pip install requests
	pip install beautifulsoup4

"""

HTTP_HEADERS = {
	'Accept-Language': 'en-US',
	'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'
}

def download_page(url, params=None):

	response = None

	try:
		response = requests.get(url, params=params, headers=HTTP_HEADERS)
		response.raise_for_status()
	except Exception as error:
		error_string = repr(error)
		print(f'Failed to download the page "{url}" with the error: {error_string}')
	
	return response

PAGE_TITLE_REGEX = re.compile(r'Go to page \d+', re.IGNORECASE)
CVE_REGEX = re.compile(r'(CVE-\d+-\d+)', re.IGNORECASE)
BUGZILLA_URL_REGEX = re.compile(r'https?://bugzilla.*', re.IGNORECASE)

VENDOR_ID_MOZILLA = 452
response = download_page('https://www.cvedetails.com/vulnerability-list.php', {'vendor_id': VENDOR_ID_MOZILLA})

if response is None:

	print(f'The script will terminate since the first Mozilla CVEs page could not be downloaded.')
	sys.exit(1)

main_soup = bs4.BeautifulSoup(response.text, 'html.parser')

page_div = main_soup.find('div', id='pagingb')
page_a_list = page_div.find_all('a', title=PAGE_TITLE_REGEX)

page_url_list = ['https://www.cvedetails.com' + page_a['href'] for page_a in page_a_list]

with open('cve-details-results.csv', 'w', newline='') as csv_file:

	csv_writer = csv.DictWriter(csv_file, fieldnames=['CVE', 'CVE Details URL', 'Bugzilla URL'])
	csv_writer.writeheader()

	for i, page_url in enumerate(page_url_list):

		print(f'Mozilla CVEs Page {i+1} of {len(page_url_list)}...')

		page_response = download_page(page_url)
		if page_response is not None:

			page_soup = bs4.BeautifulSoup(page_response.text, 'html.parser')
			vulnerability_table = page_soup.find('table', id='vulnslisttable')
			
			cve_a_list = vulnerability_table.find_all('a', title=CVE_REGEX)
		
			for j, cve_a in enumerate(cve_a_list):
		
				cve = cve_a.get_text(strip=True)
				cve_url = f'https://www.cvedetails.com/cve/{cve}'

				print(f'-> CVE {j+1} of {len(cve_a_list)}: "{cve}" from "{cve_url}"...')

				"""
				<table class="listtable" id="vulnrefstable">
					<tbody>
						<tr>
							<td class="r_average">
								<a href="https://github.com/torvalds/linux/commit/09ccfd238e5a0e670d8178cf50180ea81ae09ae1" target="_blank" title="External url">https://github.com/torvalds/linux/commit/09ccfd238e5a0e670d8178cf50180ea81ae09ae1</a>
								CONFIRM
								<br>
							</td>
						</tr>
						<tr>
							<td class="r_average">
								<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1292045" target="_blank" title="External url">https://bugzilla.redhat.com/show_bug.cgi?id=1292045</a>
								CONFIRM
								<br>
							</td>
						</tr>
					</tbody>
				</table>
				"""

				bugzilla_url = None

				cve_response = download_page(cve_url)
				cve_soup = bs4.BeautifulSoup(cve_response.text, 'html.parser')

				references_table = cve_soup.find('table', id='vulnrefstable')
				if references_table is not None:

					bugzilla_a = references_table.find('a', href=BUGZILLA_URL_REGEX)
					bugzilla_url = bugzilla_a['href'] if bugzilla_a is not None else None

				else:
					print(f'-> No references table found for {cve}.')
				
				csv_writer.writerow({'CVE': cve, 'CVE Details URL': cve_url, 'Bugzilla URL': bugzilla_url})

		print()

print('Finished running.')
