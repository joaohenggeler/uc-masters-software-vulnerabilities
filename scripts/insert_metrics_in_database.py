#!/usr/bin/env python3

"""
	This script inserts the data from any CSV files generated by "split_and_update_metrics.py" into the FILES_*, FUNCTIONS_*, CLASSES_*, and
	EXTRA_TIME_* tables in the database.

	Before running this script, the following scripts must be first run:
	- "split_and_update_metrics.py" to split and aggregate metrics by their code unit type;
	- "insert_patches_in_database.py" to insert the previously collected commits into the database;
	- "alter_functions_and_classes_in_database.py" to add the BeginLine and EndLine columns to the FUNCTIONS_* and CLASSES_* tables.
	- "alter_int_primary_keys_to_big_int_in_database.py" to modify the data type of the code unit primary keys (ID_File, ID_Function,
	and ID_Class). This prevents an error where new rows cannot be added because the maximum integer value (2147483647) is reached.

	The following file metrics are not inserted by this script:
	- DIT (MaxInheritanceTree)
	- NOC (CountClassDerived)
	- CBC (CountClassBase)
	- RFC (CountDeclMethodAll)
	- CBO (CountClassCoupled)
	- LCOM (PercentLackOfCohesion)
"""

import os
from collections import namedtuple
from typing import cast

import numpy as np # type: ignore
import pandas as pd # type: ignore

from modules.common import log, GLOBAL_CONFIG, deserialize_json_container, extract_numeric, get_list_index_or_default
from modules.database import Database
from modules.project import Project

####################################################################################################

with Database(buffered=True) as db:

	CodeUnit = namedtuple('CodeUnit', ['Kind', 'ExtraTimeTable', 'MetricsTablePrefix', 'MetricsTablePrimaryKey'])

	FILE_UNIT_INFO = 		CodeUnit('file', 		'EXTRA_TIME_FILES', 		'FILES_', 		'ID_File')
	FUNCTION_UNIT_INFO = 	CodeUnit('function', 	'EXTRA_TIME_FUNCTIONS', 	'FUNCTIONS_', 	'ID_Function')
	CLASS_UNIT_INFO = 		CodeUnit('class', 		'EXTRA_TIME_CLASS', 		'CLASSES_', 	'ID_Class')

	UNIT_INFO_LIST = [FILE_UNIT_INFO, FUNCTION_UNIT_INFO, CLASS_UNIT_INFO]

	CSV_TO_DATABASE_COLUMN = {
		# For file tables:
		'SumCountPath': 'CountPath',
		'SumCountInput': 'FanIn',
		'SumCountOutput' : 'FanOut',
		'AvgCountInput': 'AvgFanIn',
		'AvgCountOutput': 'AvgFanOut',
		'MaxCountInput': 'MaxFanIn',
		'MaxCountOutput': 'MaxFanOut',
		'HenryKafura': 'HK',
	}

	project_list = Project.get_project_list_from_config()
	for project in project_list:
		
		file_metrics_table = FILE_UNIT_INFO.MetricsTablePrefix + str(project.database_id)
		SELECT_FILE_ID_QUERY = f'''
								SELECT F.ID_File FROM {file_metrics_table} AS F
								INNER JOIN EXTRA_TIME_FILES AS E ON F.ID_File = E.ID_File
								INNER JOIN PATCHES AS P ON E.P_ID = P.P_ID
								WHERE P.R_ID = %(R_ID)s AND P.P_COMMIT = %(P_COMMIT)s
								AND F.FilePath = %(FilePath)s AND F.Occurrence = %(Occurrence)s;
								'''

		# @Hack: Doesn't handle multiple versions that were scraped at different times, though that's not really necessary for now.
		commits_csv_path = project.find_output_csv_files('affected-files')[0]
		commits = pd.read_csv(commits_csv_path, usecols=['Topological Index', 'Vulnerable Commit Hash', 'Neutral Commit Hash'], dtype=str)

		# Only neutral commits are stored in the PATCHES table. We need to convert from vulnerable to neutral commits so we can find the correct P_IDs.
		vulnerable_to_neutral_commit = {row['Vulnerable Commit Hash']: row['Neutral Commit Hash'] for _, row in commits.iterrows()}
		
		del commits_csv_path, commits

		for unit_info in UNIT_INFO_LIST:

			if not GLOBAL_CONFIG['allowed_code_units'].get(unit_info.Kind):
				log.info(f'Skipping the {unit_info.Kind} metrics for the project "{project}" at the user\'s request')
				continue

			is_function = (unit_info.Kind == 'function')
			is_class = (unit_info.Kind == 'class')
			unit_metrics_table = unit_info.MetricsTablePrefix + str(project.database_id)

			EXTRA_TIME_INSERT_QUERY = 	f'''
										INSERT INTO {unit_info.ExtraTimeTable}
										(
											P_ID, {unit_info.MetricsTablePrimaryKey}
										)
										VALUES
										(
											%(P_ID)s, %({unit_info.MetricsTablePrimaryKey})s
										);
										'''

			success, error_code = db.execute_query(f'SELECT MAX({unit_info.MetricsTablePrimaryKey} DIV 100) + 1 AS NEXT_ID FROM {unit_metrics_table};')

			assert db.cursor.rowcount != -1, 'The database cursor must be buffered.'

			next_id = -1

			if success and db.cursor.rowcount > 0:
				row = db.cursor.fetchone()
				next_id = int(row['NEXT_ID'])
				log.info(f'Found the next {unit_info.Kind} metrics ID {next_id} for the project "{project}".')
			else:
				log.error(f'Failed to find the next {unit_info.Kind} metrics ID for the project "{project}" with the error code {error_code}.')
				continue

			def get_next_unit_metrics_table_id() -> int:
				""" Retrieves the next primary key value of the ID_File, ID_Function, or ID_Class column for the current project and code unit table. """
				global next_id
				result = next_id * 100 + project.database_id
				next_id += 1
				return result

			cached_insert_queries: dict = {}

			output_csv_prefix = f'{unit_info.Kind}-metrics'
			output_csv_subdirectory = f'{unit_info.Kind}_metrics'

			def output_csv_sort_key(csv_path: str) -> int:
				""" Called when sorting the CSV list by each file path. We want units that weren't affected by a vulnerability (affected = 0, vulnerable = 0) 
				to be sorted before the vulnerable and neutral ones (affected = 1, vulnerable = 0 or 1). This is because the query that checks if the metrics
				were already inserted only works if the units whose P_ID columns will be NULL are inserted first. """
				commit_params = cast(list, extract_numeric(os.path.basename(csv_path), convert=True, all=True))
				topological_index, affected, vulnerable, *_ = commit_params # (0 to N, 0 or 1, 0 or 1)
				return topological_index * 100 + affected * 10 + vulnerable

			for input_csv_path in project.find_output_csv_files(output_csv_prefix, subdirectory=output_csv_subdirectory, sort_key=output_csv_sort_key):

				log.info(f'Inserting the {unit_info.Kind} metrics using the information in "{input_csv_path}".')

				# The CSV may be empty if a given code unit type didn't exist in a file (e.g. classes) when we split them.
				try:
					metrics = pd.read_csv(input_csv_path, dtype=str)
				except pd.errors.EmptyDataError:
					log.info(f'Skipping the empty {unit_info.Kind} metrics file "{input_csv_path}".')
					continue

				metrics = metrics.replace({np.nan: None})
				
				topological_index = metrics['Topological Index'].iloc[0]
				commit_hash = metrics['Commit Hash'].iloc[0]
				affected_commit = metrics['Affected Commit'].iloc[0] == 'Yes'
				vulnerable_commit = metrics['Vulnerable Commit'].iloc[0] == 'Yes'

				# The first commit (neutral) in the project is not in the PATCHES table, so we'll skip its metrics for now.
				if topological_index == '0':
					log.info(f'Skipping the first commit {commit_hash}.')
					continue

				commit_hash = vulnerable_to_neutral_commit.get(commit_hash, commit_hash)
				occurrence = 'before' if vulnerable_commit else 'after'

				# Since only neutral commits are stored in the PATCHES table, we need to use the occurrence (before and after a patch)
				# to determine if a commit's metrics (vulnerable or neutral) already exist in the database.
				success, error_code = db.execute_query(f'''
														SELECT
															P_ID,
															(
																SELECT COUNT(*) > 0 FROM {unit_metrics_table} AS U
																WHERE U.P_ID LIKE CONCAT('%', P.P_ID, '%') AND Occurrence = %(Occurrence)s
															) AS COMMIT_METRICS_ALREADY_EXIST
														FROM PATCHES AS P
														WHERE R_ID = %(R_ID)s AND P_COMMIT = %(P_COMMIT)s;
														''',
														params={'R_ID': project.database_id, 'P_COMMIT': commit_hash, 'Occurrence': occurrence})

				if not success:
					log.error(f'Failed to query any existing {unit_info.Kind} metrics for the commit {commit_hash} ({topological_index}, {affected_commit}, {vulnerable_commit}) in the project "{project}" with the error code {error_code}.')
					continue

				patch_row_list = [row for row in db.cursor]

				if not patch_row_list:
					log.error(f'Could not find any patch with the commit {commit_hash} ({topological_index}, {affected_commit}, {vulnerable_commit}) in the project "{project}".')
					continue

				if any(patch_row['COMMIT_METRICS_ALREADY_EXIST'] == 1 for patch_row in patch_row_list):
					
					log.info(f'Skipping the {unit_info.Kind} metrics for the patches "{patch_row_list}" with the commit {commit_hash} ({topological_index}, {affected_commit}, {vulnerable_commit}) in the project "{project}" since they already exist.')
					
					if any(patch_row['COMMIT_METRICS_ALREADY_EXIST'] == 0 for patch_row in patch_row_list):
						log.warning(f'The patches "{patch_row_list}" to be skipped have one or more {unit_info.Kind} metric values that were not previously inserted.')

					continue

				patch_list = [patch_row['P_ID'] for patch_row in patch_row_list]
				patch_list_string = '[' + ', '.join(patch_list) + ']'

				cached_file_ids: dict = {}

				##################################################

				# Remove column name spaces for itertuples().
				metrics.columns = metrics.columns.str.replace(' ', '')

				csv_metric_names = metrics.columns.values.tolist()

				first_metric_index = get_list_index_or_default(csv_metric_names, 'File') or get_list_index_or_default(csv_metric_names, 'Name') or get_list_index_or_default(csv_metric_names, 'Kind')
				first_metric_index += 1

				csv_metric_names = csv_metric_names[first_metric_index:]

				# E.g. "SumCyclomatic" -> "SumCyclomatic"" or "HenryKafura" -> "HK".
				database_metric_names = [CSV_TO_DATABASE_COLUMN.get(name, name) for name in csv_metric_names]

				# Due to the way metrics are divided by code units, the CVEs and CodeUnitLines columns may only exist
				# in CSV related to affected commits (vulnerable or neutral).
				has_code_unit_lines = 'CodeUnitLines' in metrics.columns
				has_file = 'File' in metrics.columns

				def convert_string_status_to_number(status: str) -> int:
					""" Converts a Yes/No/Unknown status from a CSV file into a numeric value used by the database. """
					if status == 'Yes':
						return 1
					elif status == 'No':
						return 0
					else:
						return 2

				##################################################
				
				for row in metrics.itertuples():

					file_path = row.File if has_file else None

					if file_path is None:
						continue

					affected_status = convert_string_status_to_number(row.VulnerableCodeUnit)

					# Possible cases for the P_ID columns in the FILES_*, FUNCTIONS_*, and CLASSES_* tables (Affected Vulnerable):
					# - (No, No) -> Set to NULL.
					# - (Yes, Yes) -> Set to the string "[P_ID_1, P_ID_2, ..., P_ID_N]", for N patches.
					# - (Yes, No) -> Set to the current P_ID, for N patches.
					p_id_list: list

					if not affected_commit:
						p_id_list = [None]
					elif affected_status != 0:
						p_id_list = [patch_list_string]
					else:
						p_id_list = patch_list

					for p_id in p_id_list:

						# Columns:
						# - File: ID_File, R_ID, P_ID, FilePath, Patched, Occurrence, Affected, [METRICS]
						# - Function: ID_Function, R_ID, P_ID, ID_Class, ID_File, Visibility, Complement, NameMethod, FilePath, Patched, Occurrence, Affected, [METRICS]
						# - Class: ID_Class, R_ID, P_ID, ID_File, Visibility, Complement, NameClass, FilePath, Patched, Occurrence, Affected, [METRICS]

						unit_id = get_next_unit_metrics_table_id()

						# Columns in common:
						query_params = {
							unit_info.MetricsTablePrimaryKey: unit_id,
							'R_ID': project.database_id,
							'P_ID':  p_id,
							'FilePath': file_path,
							'Patched': convert_string_status_to_number(row.PatchedCodeUnit), # If the code unit was changed.
							'Occurrence': occurrence, # Whether or not this code unit exists before (vulnerable) or after (neutral) the patch.
							'Affected': affected_status, # If the code unit is vulnerable or not.
						}

						if is_function or is_class:
							query_params['Visibility'] = row.Visibility
							query_params['Complement'] = row.Complement

							if has_code_unit_lines:
								lines = cast(list, deserialize_json_container(row.CodeUnitLines, [None, None]))
								query_params['BeginLine'] = lines[0]
								query_params['EndLine'] = lines[1]

							file_id = cached_file_ids.get(file_path, -1)
							if file_id == -1:

								success, error_code = db.execute_query(SELECT_FILE_ID_QUERY, params={
																		'R_ID': query_params['R_ID'],
																		'P_COMMIT': commit_hash,
																		'FilePath': query_params['FilePath'],
																		'Occurrence': query_params['Occurrence']
																		})

								if success and db.cursor.rowcount > 0:
									file_id_row = db.cursor.fetchone()
									file_id = file_id_row[FILE_UNIT_INFO.MetricsTablePrimaryKey]
								else:
									file_id = None

								cached_file_ids[file_path] = file_id

							query_params[FILE_UNIT_INFO.MetricsTablePrimaryKey] = file_id

						if is_function:
							query_params['NameMethod'] = row.Name
							query_params[CLASS_UNIT_INFO.MetricsTablePrimaryKey] = -1
						elif is_class:
							query_params['NameClass'] = row.Name

						for database_name, csv_name in zip(database_metric_names, csv_metric_names):
							query_params[database_name] = getattr(row, csv_name)
						
						query_params_key = tuple(query_params.keys())
						query = cached_insert_queries.get(query_params_key)

						if query is None:

							query = f'INSERT INTO {unit_metrics_table} ('

							for name in query_params:
								query += f'{name},'

							query = query.rstrip(',')
							query += ') VALUES ('

							for name in query_params:
								query += f'%({name})s,'

							query = query.rstrip(',')
							query += ');'

							cached_insert_queries[query_params_key] = query

						success, error_code = db.execute_query(query, params=query_params)

						def insert_patch_and_unit_ids_in_extra_time_table(patch_id: str) -> None:
							""" Inserts a row for the current code unit into the appropriate EXTRA_TIME_* table. """

							success, error_code = db.execute_query(EXTRA_TIME_INSERT_QUERY,
																	params={
																		'P_ID': patch_id,
																		unit_info.MetricsTablePrimaryKey: unit_id,
																	}
																)

							if not success:
								log.error(f'Failed to insert the {unit_info.Kind} metrics ID in the {unit_info.ExtraTimeTable} table for the unit "{row.Name}" ({unit_id}) in the file "{file_path}" and commit {commit_hash} ({topological_index}, {patch_id}, {affected_commit}, {vulnerable_commit}) with the error code {error_code}.')

						if success:
							if affected_commit and affected_status == 0:
								insert_patch_and_unit_ids_in_extra_time_table(p_id)
							else:
								for p_id in patch_list:
									insert_patch_and_unit_ids_in_extra_time_table(p_id)
						else:
							log.error(f'Failed to insert the {unit_info.Kind} metrics for the unit "{row.Name}" ({unit_id}) in the file "{file_path}" and commit {commit_hash} ({topological_index}, {p_id}, {affected_commit}, {vulnerable_commit}) with the error code {error_code}.')

				##################################################

				db.commit()

log.info('Finished running.')
print('Finished running.')
